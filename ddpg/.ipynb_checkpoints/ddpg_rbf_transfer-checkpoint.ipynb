{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from actor import ActorNetwork\n",
    "from rbfActor import RbfActorNetwork\n",
    "from actorls import ActorLSNetwork\n",
    "from critic import CriticNetwork\n",
    "from replay_buffer import ReplayBuffer\n",
    "from ounoise import OUNoise\n",
    "import gym, time\n",
    "from Envs.reaching import ReachingEnv\n",
    "from Envs.throwing import ThrowingEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensor_toolbox_yyang as ttool\n",
    "\n",
    "MAX_EPISODE = 500\n",
    "MAX_TIME = 200\n",
    "\n",
    "ACTOR_LEARNING_RATE = 0.0001\n",
    "CRITIC_LEARNING_RATE = 0.001\n",
    "L2_DECAY = 0.01\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "\n",
    "BUFFER_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "OU_MU = 0.0\n",
    "OU_THETA = 0.15  \n",
    "OU_SIGMA = 0.20\n",
    "\n",
    "RANDOM_SEED = 1926\n",
    "\n",
    "goal_pos = np.load('./Envs/reaching_goal_pos.npy')\n",
    "source_paras = np.load('./Data/reaching_ddpg_rbf.npz')\n",
    "\n",
    "GAMMA = .99\n",
    "# env = gym.make('Pendulum-v0')\n",
    "env = ReachingEnv(include_t = True)\n",
    "# env = ThrowingEnv(include_t = True)\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = env.action_space.high\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "all_weights = np.array([v[0] for v in source_paras['arr_0'][10:]])\n",
    "rank = [10,50,2]\n",
    "U, S = ttool.tucker_dcmp(all_weights, eps_or_k = rank)\n",
    "temp = np.tensordot(S, U[1], axes = (1,-1))\n",
    "L = np.tensordot(temp,U[2], axes = (1,-1))\n",
    "S = U[0][0]\n",
    "S = np.expand_dims(S, axis = -1)\n",
    "S = np.expand_dims(S, axis = -1)\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 501) (501, 2)\n",
      "(?, 501) (501, 2)\n"
     ]
    }
   ],
   "source": [
    "# actor = ActorNetwork(sess, state_dim, action_dim, action_bound, hidden_layer_dim = [40,30], \\\n",
    "#                      seed = RANDOM_SEED, tau = TAU, learning_rate = ACTOR_LEARNING_RATE)\n",
    "actor = ActorLSNetwork(sess, state_dim, action_dim, action_bound, L_init = L, S_init = S, \\\n",
    "                     seed = RANDOM_SEED, tau = TAU, learning_rate = ACTOR_LEARNING_RATE)\n",
    "\n",
    "critic = CriticNetwork(sess, state_dim, action_dim, hidden_layer_dim = [30],\\\n",
    "                       l2_alpha = L2_DECAY, seed = RANDOM_SEED, tau =TAU, learning_rate = CRITIC_LEARNING_RATE)\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "noise = OUNoise(action_dim, mu = OU_MU, theta = OU_THETA, sigma = OU_SIGMA, seed = RANDOM_SEED)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "summary_sum_reward_list = np.zeros([50,MAX_EPISODE])\n",
    "summary_avg_reward_list = np.zeros([50,MAX_EPISODE])\n",
    "all_recorded_actor_paras = []\n",
    "all_recorded_critic_paras = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task : 0 \t Episode: 0 \t Time_step: 199 \t Avg_reward: -34.0833846454 \t Cur_reward: -34.0833846454\n",
      "Task : 0 \t Episode: 1 \t Time_step: 199 \t Avg_reward: -60.6138298235 \t Cur_reward: -87.1442750016\n",
      "Task : 0 \t Episode: 2 \t Time_step: 199 \t Avg_reward: -96.726657684 \t Cur_reward: -168.952313405\n",
      "Task : 0 \t Episode: 3 \t Time_step: 199 \t Avg_reward: -109.292973954 \t Cur_reward: -146.991922766\n",
      "Task : 0 \t Episode: 4 \t Time_step: 199 \t Avg_reward: -113.511950434 \t Cur_reward: -130.387856354\n",
      "Task : 0 \t Episode: 5 \t Time_step: 199 \t Avg_reward: -106.975193399 \t Cur_reward: -74.291408225\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(goal_pos)-40):\n",
    "    env.set_goal(np.append(goal_pos[j], [0,0]))\n",
    "    all_sum_reward_list = []\n",
    "    all_avg_reward_list = []\n",
    "    all_reward_list = []\n",
    "    all_loss_list = []\n",
    "    all_t_list = []\n",
    "    \n",
    "    for i in range(MAX_EPISODE):\n",
    "\n",
    "        state = env.reset()\n",
    "        noise.reset()\n",
    "        reward_list = []\n",
    "        loss_list = []\n",
    "\n",
    "        for t in range(MAX_TIME):\n",
    "            action = actor.predict(np.reshape(state, (-1, state_dim))) \n",
    "            action += noise.noise()\n",
    "            action = np.clip(action, -action_bound, action_bound)\n",
    "            action = np.reshape(action, action_dim)\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            replay_buffer.add_sample(np.reshape(state, state_dim), \\\n",
    "                                     np.reshape(action,action_dim),\\\n",
    "                                     reward,\\\n",
    "                                     np.reshape(next_state,state_dim),\\\n",
    "                                     done)\n",
    "\n",
    "            mini_batch = replay_buffer.rand_sample(batch_size = BATCH_SIZE, seed = RANDOM_SEED + t + i*MAX_TIME)\n",
    "            s_batch, a_batch, r_batch, s2_batch, t_batch = mini_batch\n",
    "\n",
    "            a2_batch = actor.predict(s2_batch, if_target = True)\n",
    "            training_q = r_batch + GAMMA * critic.predict(s2_batch, a2_batch, if_target = True) #* ~t_batch\n",
    "\n",
    "            _, loss = critic.train(s_batch, a_batch, training_q)\n",
    "\n",
    "            train_action_batch = actor.predict(s_batch)\n",
    "            critic_grad = critic.compute_critic_gradient(s_batch, train_action_batch)\n",
    "            actor.train(s_batch, critic_grad[0])\n",
    "\n",
    "            actor.update_target_network()\n",
    "            critic.update_target_network()\n",
    "\n",
    "            reward_list.append(reward)\n",
    "            loss_list.append(loss)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    #         print('Episode: %s \\t Action: %s, %s \\t State: %s,%s,%s,%s' %(i, action[0], action[1], state[0],state[1],state[2],state[3]))\n",
    "\n",
    "        all_sum_reward_list.append(np.sum(reward_list))\n",
    "        all_avg_reward_list.append(np.mean(all_sum_reward_list[-100:]))\n",
    "        all_reward_list.append(reward_list)\n",
    "        all_loss_list.append(loss_list)\n",
    "        all_t_list.append(t)\n",
    "\n",
    "        print('Task : %s \\t Episode: %s \\t Time_step: %s \\t Avg_reward: %s \\t Cur_reward: %s'%(j, i, t, all_avg_reward_list[-1], all_sum_reward_list[-1]))\n",
    "    \n",
    "    summary_sum_reward_list[j] = np.array(all_sum_reward_list)\n",
    "    summary_avg_reward_list[j] = np.array(all_avg_reward_list)\n",
    "    record_paras = [v for v in tf.trainable_variables() if 'actor_target' in v.name]\n",
    "    all_recorded_paras.append(sess.run(record_paras))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10)\n",
      "(501, 50)\n",
      "(2, 2)\n",
      "(10, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "[print(v.shape) for v in U]\n",
    "print(S.shape)\n",
    "\n",
    "test = np.tensordot(S, U[1], axes = (1,-1))\n",
    "print(test.shape)\n",
    "\n",
    "test2 = np.tensordot(test, U[2], axes = (1,-1))\n",
    "print(test2.shape)\n",
    "\n",
    "test3 = np.array([ np.sum(np.array([ U[0][v][i]*test2[i]  for i in range(10)]), axis = 0) for v in range(50)])\n",
    "# test3 = np.tensordot(test2,U[0], axes = (-1,0))\n",
    "# print(test3.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
