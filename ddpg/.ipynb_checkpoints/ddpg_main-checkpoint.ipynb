{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from actor import ActorNetwork\n",
    "from rbfActor import RbfActorNetwork\n",
    "from critic import CriticNetwork\n",
    "from replay_buffer import ReplayBuffer\n",
    "from ounoise import OUNoise\n",
    "import gym, time\n",
    "from Envs.reaching import ReachingEnv\n",
    "from Envs.throwing import ThrowingEnv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "MAX_EPISODE = 250\n",
    "MAX_TIME = 200\n",
    "\n",
    "ACTOR_LEARNING_RATE = 0.0001\n",
    "CRITIC_LEARNING_RATE = 0.001\n",
    "L2_DECAY = 0.01\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "\n",
    "BUFFER_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "OU_MU = 0.0\n",
    "OU_THETA = 0.15  \n",
    "OU_SIGMA = 0.20\n",
    "\n",
    "RANDOM_SEED = 1926\n",
    "\n",
    "goal_pos = np.load('./Envs/reaching_goal_pos.npy')\n",
    "\n",
    "# goal_pos = np.array([[-4.11399563, -5.        ],\n",
    "#        [-0.05680097, -5.        ],\n",
    "#        [ 3.51188653, -5.        ],\n",
    "#        [ 2.25174116, -5.        ],\n",
    "#        [ 4.24114159, -5.        ],\n",
    "#        [-3.44134834, -5.        ],\n",
    "#        [-2.44153671, -5.        ],\n",
    "#        [-2.33641164, -5.        ],\n",
    "#        [-2.77225586, -5.        ],\n",
    "#        [-1.0171196 , -5.        ]])\n",
    "GAMMA = .99\n",
    "# env = gym.make('Pendulum-v0')\n",
    "env = ReachingEnv(include_t = True)\n",
    "# env = ThrowingEnv(include_t = True)\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = env.action_space.high\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "actor = ActorNetwork(sess, state_dim, action_dim, action_bound, hidden_layer_dim = [40,30], \\\n",
    "                     seed = RANDOM_SEED, tau = TAU, learning_rate = ACTOR_LEARNING_RATE)\n",
    "# actor = RbfActorNetwork(sess, state_dim, action_dim, action_bound, hidden_layer_dim = [40,30], \\\n",
    "#                      seed = RANDOM_SEED, tau = TAU, learning_rate = ACTOR_LEARNING_RATE)\n",
    "\n",
    "critic = CriticNetwork(sess, state_dim, action_dim, hidden_layer_dim = [30],\\\n",
    "                       l2_alpha = L2_DECAY, seed = RANDOM_SEED, tau =TAU, learning_rate = CRITIC_LEARNING_RATE)\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "noise = OUNoise(action_dim, mu = OU_MU, theta = OU_THETA, sigma = OU_SIGMA, seed = RANDOM_SEED)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "summary_sum_reward_list = np.zeros([50,MAX_EPISODE])\n",
    "summary_avg_reward_list = np.zeros([50,MAX_EPISODE])\n",
    "all_recorded_paras = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task : 0 \t Episode: 0 \t Time_step: 199 \t Avg_reward: -75.1653294911 \t Cur_reward: -75.1653294911\n",
      "Task : 0 \t Episode: 1 \t Time_step: 199 \t Avg_reward: -69.9493169292 \t Cur_reward: -64.7333043674\n",
      "Task : 0 \t Episode: 2 \t Time_step: 199 \t Avg_reward: -75.4527745665 \t Cur_reward: -86.4596898412\n",
      "Task : 0 \t Episode: 3 \t Time_step: 199 \t Avg_reward: -63.9215298592 \t Cur_reward: -29.3277957371\n",
      "Task : 0 \t Episode: 4 \t Time_step: 199 \t Avg_reward: -58.2973255487 \t Cur_reward: -35.8005083067\n",
      "Task : 0 \t Episode: 5 \t Time_step: 199 \t Avg_reward: -55.6001476627 \t Cur_reward: -42.1142582328\n",
      "Task : 0 \t Episode: 6 \t Time_step: 199 \t Avg_reward: -55.2644756132 \t Cur_reward: -53.250443316\n",
      "Task : 0 \t Episode: 7 \t Time_step: 199 \t Avg_reward: -50.297467106 \t Cur_reward: -15.5284075555\n",
      "Task : 0 \t Episode: 8 \t Time_step: 199 \t Avg_reward: -47.7056467876 \t Cur_reward: -26.9710842406\n",
      "Task : 0 \t Episode: 9 \t Time_step: 199 \t Avg_reward: -44.2156626974 \t Cur_reward: -12.8058058861\n",
      "Task : 0 \t Episode: 10 \t Time_step: 199 \t Avg_reward: -41.1486872011 \t Cur_reward: -10.4789322376\n",
      "Task : 0 \t Episode: 11 \t Time_step: 199 \t Avg_reward: -38.4470742637 \t Cur_reward: -8.72933195183\n",
      "Task : 0 \t Episode: 12 \t Time_step: 199 \t Avg_reward: -36.1262534576 \t Cur_reward: -8.27640378485\n",
      "Task : 0 \t Episode: 13 \t Time_step: 199 \t Avg_reward: -34.3885942911 \t Cur_reward: -11.799025127\n",
      "Task : 0 \t Episode: 14 \t Time_step: 199 \t Avg_reward: -32.6209525214 \t Cur_reward: -7.87396774593\n",
      "Task : 0 \t Episode: 15 \t Time_step: 199 \t Avg_reward: -31.3276841731 \t Cur_reward: -11.9286589485\n",
      "Task : 0 \t Episode: 16 \t Time_step: 199 \t Avg_reward: -36.1015375948 \t Cur_reward: -112.483192341\n",
      "Task : 0 \t Episode: 17 \t Time_step: 199 \t Avg_reward: -35.2657871405 \t Cur_reward: -21.0580294187\n",
      "Task : 0 \t Episode: 18 \t Time_step: 199 \t Avg_reward: -34.3569754247 \t Cur_reward: -17.9983645393\n",
      "Task : 0 \t Episode: 19 \t Time_step: 199 \t Avg_reward: -33.273703314 \t Cur_reward: -12.6915332118\n",
      "Task : 0 \t Episode: 20 \t Time_step: 199 \t Avg_reward: -32.46155843 \t Cur_reward: -16.2186607488\n",
      "Task : 0 \t Episode: 21 \t Time_step: 199 \t Avg_reward: -31.7758278023 \t Cur_reward: -17.3754846216\n",
      "Task : 0 \t Episode: 22 \t Time_step: 199 \t Avg_reward: -31.5217798909 \t Cur_reward: -25.9327258402\n",
      "Task : 0 \t Episode: 23 \t Time_step: 199 \t Avg_reward: -31.1579624913 \t Cur_reward: -22.7901623002\n",
      "Task : 0 \t Episode: 24 \t Time_step: 199 \t Avg_reward: -30.9891592823 \t Cur_reward: -26.9378822649\n",
      "Task : 0 \t Episode: 25 \t Time_step: 199 \t Avg_reward: -30.6211008394 \t Cur_reward: -21.4196397668\n",
      "Task : 0 \t Episode: 26 \t Time_step: 199 \t Avg_reward: -29.9619456581 \t Cur_reward: -12.8239109456\n",
      "Task : 0 \t Episode: 27 \t Time_step: 199 \t Avg_reward: -29.5261470767 \t Cur_reward: -17.7595853776\n",
      "Task : 0 \t Episode: 28 \t Time_step: 199 \t Avg_reward: -29.0024491757 \t Cur_reward: -14.3389079486\n",
      "Task : 0 \t Episode: 29 \t Time_step: 199 \t Avg_reward: -28.8350296251 \t Cur_reward: -23.9798626569\n",
      "Task : 0 \t Episode: 30 \t Time_step: 199 \t Avg_reward: -28.5300378618 \t Cur_reward: -19.3802849639\n",
      "Task : 0 \t Episode: 31 \t Time_step: 199 \t Avg_reward: -28.8344197456 \t Cur_reward: -38.2702581419\n",
      "Task : 0 \t Episode: 32 \t Time_step: 199 \t Avg_reward: -28.2251727369 \t Cur_reward: -8.72926845837\n",
      "Task : 0 \t Episode: 33 \t Time_step: 199 \t Avg_reward: -31.4357282004 \t Cur_reward: -137.384058498\n",
      "Task : 0 \t Episode: 34 \t Time_step: 199 \t Avg_reward: -31.5960371882 \t Cur_reward: -37.0465427731\n",
      "Task : 0 \t Episode: 35 \t Time_step: 199 \t Avg_reward: -31.0289862182 \t Cur_reward: -11.182202266\n",
      "Task : 0 \t Episode: 36 \t Time_step: 199 \t Avg_reward: -30.4528343204 \t Cur_reward: -9.71136600145\n",
      "Task : 0 \t Episode: 37 \t Time_step: 199 \t Avg_reward: -30.3491639472 \t Cur_reward: -26.5133601385\n",
      "Task : 0 \t Episode: 38 \t Time_step: 199 \t Avg_reward: -32.0548135213 \t Cur_reward: -96.8694973386\n",
      "Task : 0 \t Episode: 39 \t Time_step: 199 \t Avg_reward: -33.2166953041 \t Cur_reward: -78.5300848316\n",
      "Task : 0 \t Episode: 40 \t Time_step: 199 \t Avg_reward: -34.0788490249 \t Cur_reward: -68.5649978581\n",
      "Task : 0 \t Episode: 41 \t Time_step: 199 \t Avg_reward: -34.5019252971 \t Cur_reward: -51.8480524568\n",
      "Task : 0 \t Episode: 42 \t Time_step: 199 \t Avg_reward: -34.5927056323 \t Cur_reward: -38.4054797102\n",
      "Task : 0 \t Episode: 43 \t Time_step: 199 \t Avg_reward: -34.2686167805 \t Cur_reward: -20.3327961549\n",
      "Task : 0 \t Episode: 44 \t Time_step: 199 \t Avg_reward: -34.5684594161 \t Cur_reward: -47.761535382\n",
      "Task : 0 \t Episode: 45 \t Time_step: 199 \t Avg_reward: -34.3358713306 \t Cur_reward: -23.8694074811\n",
      "Task : 0 \t Episode: 46 \t Time_step: 199 \t Avg_reward: -36.0497135183 \t Cur_reward: -114.886454155\n",
      "Task : 0 \t Episode: 47 \t Time_step: 199 \t Avg_reward: -36.1684572076 \t Cur_reward: -41.7494106035\n",
      "Task : 0 \t Episode: 48 \t Time_step: 199 \t Avg_reward: -35.6791994487 \t Cur_reward: -12.1948270217\n",
      "Task : 0 \t Episode: 49 \t Time_step: 199 \t Avg_reward: -35.1954527362 \t Cur_reward: -11.491863825\n",
      "Task : 0 \t Episode: 50 \t Time_step: 199 \t Avg_reward: -34.7946266169 \t Cur_reward: -14.7533206506\n",
      "Task : 0 \t Episode: 51 \t Time_step: 199 \t Avg_reward: -34.4749607696 \t Cur_reward: -18.1720025572\n",
      "Task : 0 \t Episode: 52 \t Time_step: 199 \t Avg_reward: -34.1760175232 \t Cur_reward: -18.6309687108\n",
      "Task : 0 \t Episode: 53 \t Time_step: 199 \t Avg_reward: -33.7199622814 \t Cur_reward: -9.54903446409\n",
      "Task : 0 \t Episode: 54 \t Time_step: 199 \t Avg_reward: -33.4588701342 \t Cur_reward: -19.3598941891\n",
      "Task : 0 \t Episode: 55 \t Time_step: 199 \t Avg_reward: -33.2535902361 \t Cur_reward: -21.9631958386\n",
      "Task : 0 \t Episode: 56 \t Time_step: 199 \t Avg_reward: -32.8962197356 \t Cur_reward: -12.8834717076\n",
      "Task : 0 \t Episode: 57 \t Time_step: 199 \t Avg_reward: -32.8496419016 \t Cur_reward: -30.1947053603\n",
      "Task : 0 \t Episode: 58 \t Time_step: 199 \t Avg_reward: -32.5710084545 \t Cur_reward: -16.4102685247\n",
      "Task : 0 \t Episode: 59 \t Time_step: 199 \t Avg_reward: -32.2543376731 \t Cur_reward: -13.5707615741\n",
      "Task : 0 \t Episode: 60 \t Time_step: 199 \t Avg_reward: -31.9118872466 \t Cur_reward: -11.3648616528\n",
      "Task : 0 \t Episode: 61 \t Time_step: 199 \t Avg_reward: -31.6374185737 \t Cur_reward: -14.8948295251\n",
      "Task : 0 \t Episode: 62 \t Time_step: 199 \t Avg_reward: -31.4140678378 \t Cur_reward: -17.5663222131\n",
      "Task : 0 \t Episode: 63 \t Time_step: 199 \t Avg_reward: -31.1220089722 \t Cur_reward: -12.7223004388\n",
      "Task : 0 \t Episode: 64 \t Time_step: 199 \t Avg_reward: -30.8024756643 \t Cur_reward: -10.3523439607\n",
      "Task : 0 \t Episode: 65 \t Time_step: 199 \t Avg_reward: -30.5065218133 \t Cur_reward: -11.2695214967\n",
      "Task : 0 \t Episode: 66 \t Time_step: 199 \t Avg_reward: -30.2705955292 \t Cur_reward: -14.6994607771\n",
      "Task : 0 \t Episode: 67 \t Time_step: 199 \t Avg_reward: -29.9961859531 \t Cur_reward: -11.6107443546\n",
      "Task : 0 \t Episode: 68 \t Time_step: 199 \t Avg_reward: -29.7701334165 \t Cur_reward: -14.3985609331\n",
      "Task : 0 \t Episode: 69 \t Time_step: 199 \t Avg_reward: -29.5376310988 \t Cur_reward: -13.4949711774\n",
      "Task : 0 \t Episode: 70 \t Time_step: 199 \t Avg_reward: -29.265457879 \t Cur_reward: -10.2133324931\n",
      "Task : 0 \t Episode: 71 \t Time_step: 199 \t Avg_reward: -28.9985012382 \t Cur_reward: -10.0445797357\n",
      "Task : 0 \t Episode: 72 \t Time_step: 199 \t Avg_reward: -28.847176041 \t Cur_reward: -17.9517618463\n",
      "Task : 0 \t Episode: 73 \t Time_step: 199 \t Avg_reward: -28.6598779129 \t Cur_reward: -14.9871145633\n",
      "Task : 0 \t Episode: 74 \t Time_step: 199 \t Avg_reward: -28.3827286994 \t Cur_reward: -7.87368689521\n",
      "Task : 0 \t Episode: 75 \t Time_step: 199 \t Avg_reward: -28.1750676672 \t Cur_reward: -12.6004902565\n",
      "Task : 0 \t Episode: 76 \t Time_step: 199 \t Avg_reward: -27.9183140096 \t Cur_reward: -8.40503603363\n",
      "Task : 0 \t Episode: 77 \t Time_step: 199 \t Avg_reward: -27.6469062915 \t Cur_reward: -6.74851199312\n",
      "Task : 0 \t Episode: 78 \t Time_step: 199 \t Avg_reward: -27.4053417402 \t Cur_reward: -8.56330674263\n",
      "Task : 0 \t Episode: 79 \t Time_step: 199 \t Avg_reward: -27.1684834279 \t Cur_reward: -8.45667675499\n",
      "Task : 0 \t Episode: 80 \t Time_step: 199 \t Avg_reward: -27.001350922 \t Cur_reward: -13.630750451\n",
      "Task : 0 \t Episode: 81 \t Time_step: 199 \t Avg_reward: -26.7752652116 \t Cur_reward: -8.4623226663\n",
      "Task : 0 \t Episode: 82 \t Time_step: 199 \t Avg_reward: -26.5688645418 \t Cur_reward: -9.64400962184\n",
      "Task : 0 \t Episode: 83 \t Time_step: 199 \t Avg_reward: -26.4159765575 \t Cur_reward: -13.7262738576\n",
      "Task : 0 \t Episode: 84 \t Time_step: 199 \t Avg_reward: -26.1932391857 \t Cur_reward: -7.48329995534\n",
      "Task : 0 \t Episode: 85 \t Time_step: 199 \t Avg_reward: -26.0359462652 \t Cur_reward: -12.6660480249\n",
      "Task : 0 \t Episode: 86 \t Time_step: 199 \t Avg_reward: -25.8742481926 \t Cur_reward: -11.968213945\n",
      "Task : 0 \t Episode: 87 \t Time_step: 199 \t Avg_reward: -25.6828546888 \t Cur_reward: -9.03161986054\n",
      "Task : 0 \t Episode: 88 \t Time_step: 199 \t Avg_reward: -25.4860844517 \t Cur_reward: -8.17030358394\n",
      "Task : 0 \t Episode: 89 \t Time_step: 199 \t Avg_reward: -25.3621634843 \t Cur_reward: -14.3331973887\n",
      "Task : 0 \t Episode: 90 \t Time_step: 199 \t Avg_reward: -25.17920108 \t Cur_reward: -8.71258469254\n",
      "Task : 0 \t Episode: 91 \t Time_step: 199 \t Avg_reward: -24.9925007373 \t Cur_reward: -8.00276955366\n",
      "Task : 0 \t Episode: 92 \t Time_step: 199 \t Avg_reward: -24.7985687191 \t Cur_reward: -6.95682304606\n",
      "Task : 0 \t Episode: 93 \t Time_step: 199 \t Avg_reward: -24.5877853292 \t Cur_reward: -4.98493006019\n",
      "Task : 0 \t Episode: 94 \t Time_step: 199 \t Avg_reward: -24.4070321756 \t Cur_reward: -7.41623574396\n",
      "Task : 0 \t Episode: 95 \t Time_step: 199 \t Avg_reward: -24.2347055086 \t Cur_reward: -7.86367213673\n",
      "Task : 0 \t Episode: 96 \t Time_step: 199 \t Avg_reward: -24.0776263963 \t Cur_reward: -8.9980316239\n",
      "Task : 0 \t Episode: 97 \t Time_step: 199 \t Avg_reward: -23.9210392669 \t Cur_reward: -8.73208771459\n",
      "Task : 0 \t Episode: 98 \t Time_step: 199 \t Avg_reward: -23.7430735944 \t Cur_reward: -6.3024376887\n",
      "Task : 0 \t Episode: 99 \t Time_step: 199 \t Avg_reward: -23.5935427966 \t Cur_reward: -8.78999381366\n",
      "Task : 0 \t Episode: 100 \t Time_step: 199 \t Avg_reward: -22.9006466351 \t Cur_reward: -5.87571333904\n",
      "Task : 0 \t Episode: 101 \t Time_step: 199 \t Avg_reward: -22.3084751281 \t Cur_reward: -5.51615366501\n",
      "Task : 0 \t Episode: 102 \t Time_step: 199 \t Avg_reward: -21.503916439 \t Cur_reward: -6.0038209325\n",
      "Task : 0 \t Episode: 103 \t Time_step: 199 \t Avg_reward: -21.2699369638 \t Cur_reward: -5.92984821765\n",
      "Task : 0 \t Episode: 104 \t Time_step: 199 \t Avg_reward: -20.9684219542 \t Cur_reward: -5.64900734266\n",
      "Task : 0 \t Episode: 105 \t Time_step: 199 \t Avg_reward: -20.6103534456 \t Cur_reward: -6.30740737432\n",
      "Task : 0 \t Episode: 106 \t Time_step: 199 \t Avg_reward: -20.1464014827 \t Cur_reward: -6.85524702657\n",
      "Task : 0 \t Episode: 107 \t Time_step: 199 \t Avg_reward: -20.0509416708 \t Cur_reward: -5.98242636869\n",
      "Task : 0 \t Episode: 108 \t Time_step: 199 \t Avg_reward: -19.8273186829 \t Cur_reward: -4.60878545094\n",
      "Task : 0 \t Episode: 109 \t Time_step: 199 \t Avg_reward: -19.7447217472 \t Cur_reward: -4.54611231419\n",
      "Task : 0 \t Episode: 110 \t Time_step: 199 \t Avg_reward: -19.7092266123 \t Cur_reward: -6.92941874637\n",
      "Task : 0 \t Episode: 111 \t Time_step: 199 \t Avg_reward: -19.6737112692 \t Cur_reward: -5.17779764454\n",
      "Task : 0 \t Episode: 112 \t Time_step: 199 \t Avg_reward: -19.6715135369 \t Cur_reward: -8.05663055435\n",
      "Task : 0 \t Episode: 113 \t Time_step: 199 \t Avg_reward: -19.6137160126 \t Cur_reward: -6.01927269793\n",
      "Task : 0 \t Episode: 114 \t Time_step: 199 \t Avg_reward: -19.5936829159 \t Cur_reward: -5.87065807681\n",
      "Task : 0 \t Episode: 115 \t Time_step: 199 \t Avg_reward: -19.5255095805 \t Cur_reward: -5.11132540367\n",
      "Task : 0 \t Episode: 116 \t Time_step: 199 \t Avg_reward: -18.445791259 \t Cur_reward: -4.51136019413\n",
      "Task : 0 \t Episode: 117 \t Time_step: 199 \t Avg_reward: -18.2795730982 \t Cur_reward: -4.43621334289\n",
      "Task : 0 \t Episode: 118 \t Time_step: 199 \t Avg_reward: -18.1779474028 \t Cur_reward: -7.83579499707\n",
      "Task : 0 \t Episode: 119 \t Time_step: 199 \t Avg_reward: -18.1063329615 \t Cur_reward: -5.53008907477\n",
      "Task : 0 \t Episode: 120 \t Time_step: 199 \t Avg_reward: -18.0110601807 \t Cur_reward: -6.69138267126\n",
      "Task : 0 \t Episode: 121 \t Time_step: 199 \t Avg_reward: -17.9069597005 \t Cur_reward: -6.9654366041\n",
      "Task : 0 \t Episode: 122 \t Time_step: 199 \t Avg_reward: -17.7118603071 \t Cur_reward: -6.42278649661\n",
      "Task : 0 \t Episode: 123 \t Time_step: 199 \t Avg_reward: -17.5555477853 \t Cur_reward: -7.15891011959\n",
      "Task : 0 \t Episode: 124 \t Time_step: 199 \t Avg_reward: -17.3882885591 \t Cur_reward: -10.2119596453\n",
      "Task : 0 \t Episode: 125 \t Time_step: 199 \t Avg_reward: -17.3017096827 \t Cur_reward: -12.7617521337\n",
      "Task : 0 \t Episode: 126 \t Time_step: 199 \t Avg_reward: -17.2376510534 \t Cur_reward: -6.41804801602\n",
      "Task : 0 \t Episode: 127 \t Time_step: 199 \t Avg_reward: -17.1446520552 \t Cur_reward: -8.45968555674\n",
      "Task : 0 \t Episode: 128 \t Time_step: 199 \t Avg_reward: -17.0830402691 \t Cur_reward: -8.17772933384\n",
      "Task : 0 \t Episode: 129 \t Time_step: 199 \t Avg_reward: -16.9407617582 \t Cur_reward: -9.75201157257\n",
      "Task : 0 \t Episode: 130 \t Time_step: 199 \t Avg_reward: -16.8323960441 \t Cur_reward: -8.5437135521\n",
      "Task : 0 \t Episode: 131 \t Time_step: 199 \t Avg_reward: -16.5419054711 \t Cur_reward: -9.22120084237\n",
      "Task : 0 \t Episode: 132 \t Time_step: 199 \t Avg_reward: -16.5602416397 \t Cur_reward: -10.5628853187\n",
      "Task : 0 \t Episode: 133 \t Time_step: 199 \t Avg_reward: -15.2771363951 \t Cur_reward: -9.07353403401\n",
      "Task : 0 \t Episode: 134 \t Time_step: 199 \t Avg_reward: -15.0172217737 \t Cur_reward: -11.055080634\n",
      "Task : 0 \t Episode: 135 \t Time_step: 199 \t Avg_reward: -15.0159025933 \t Cur_reward: -11.0502842228\n",
      "Task : 0 \t Episode: 136 \t Time_step: 199 \t Avg_reward: -15.0501107793 \t Cur_reward: -13.1321846085\n",
      "Task : 0 \t Episode: 137 \t Time_step: 199 \t Avg_reward: -14.9370266392 \t Cur_reward: -15.2049461297\n",
      "Task : 0 \t Episode: 138 \t Time_step: 199 \t Avg_reward: -14.0707777954 \t Cur_reward: -10.2446129535\n",
      "Task : 0 \t Episode: 139 \t Time_step: 199 \t Avg_reward: -13.3866095975 \t Cur_reward: -10.1132650393\n",
      "Task : 0 \t Episode: 140 \t Time_step: 199 \t Avg_reward: -12.820997507 \t Cur_reward: -12.0037888074\n",
      "Task : 0 \t Episode: 141 \t Time_step: 199 \t Avg_reward: -12.4193736118 \t Cur_reward: -11.6856629454\n",
      "Task : 0 \t Episode: 142 \t Time_step: 199 \t Avg_reward: -12.1704874041 \t Cur_reward: -13.5168589346\n",
      "Task : 0 \t Episode: 143 \t Time_step: 199 \t Avg_reward: -12.0601786932 \t Cur_reward: -9.30192506645\n",
      "Task : 0 \t Episode: 144 \t Time_step: 199 \t Avg_reward: -11.649428089 \t Cur_reward: -6.68647496053\n",
      "Task : 0 \t Episode: 145 \t Time_step: 199 \t Avg_reward: -11.4884768144 \t Cur_reward: -7.77428001905\n",
      "Task : 0 \t Episode: 146 \t Time_step: 199 \t Avg_reward: -10.4421558609 \t Cur_reward: -10.2543588067\n",
      "Task : 0 \t Episode: 147 \t Time_step: 199 \t Avg_reward: -10.1137625577 \t Cur_reward: -8.91008028734\n",
      "Task : 0 \t Episode: 148 \t Time_step: 199 \t Avg_reward: -10.0848053997 \t Cur_reward: -9.29911121487\n",
      "Task : 0 \t Episode: 149 \t Time_step: 199 \t Avg_reward: -10.0730003888 \t Cur_reward: -10.311362738\n",
      "Task : 0 \t Episode: 150 \t Time_step: 199 \t Avg_reward: -9.99421589516 \t Cur_reward: -6.87487128751\n",
      "Task : 0 \t Episode: 151 \t Time_step: 199 \t Avg_reward: -9.89009670566 \t Cur_reward: -7.76008360713\n",
      "Task : 0 \t Episode: 152 \t Time_step: 199 \t Avg_reward: -9.75984666295 \t Cur_reward: -5.60596443962\n",
      "Task : 0 \t Episode: 153 \t Time_step: 199 \t Avg_reward: -9.73358433524 \t Cur_reward: -6.92280169295\n",
      "Task : 0 \t Episode: 154 \t Time_step: 199 \t Avg_reward: -9.61563862362 \t Cur_reward: -7.5653230273\n",
      "Task : 0 \t Episode: 155 \t Time_step: 199 \t Avg_reward: -9.48288212161 \t Cur_reward: -8.68754563744\n",
      "Task : 0 \t Episode: 156 \t Time_step: 199 \t Avg_reward: -9.44640141179 \t Cur_reward: -9.23540072563\n",
      "Task : 0 \t Episode: 157 \t Time_step: 199 \t Avg_reward: -9.21241406423 \t Cur_reward: -6.79597060463\n",
      "Task : 0 \t Episode: 158 \t Time_step: 199 \t Avg_reward: -9.1268855962 \t Cur_reward: -7.85742172172\n",
      "Task : 0 \t Episode: 159 \t Time_step: 199 \t Avg_reward: -9.06413994248 \t Cur_reward: -7.29619620235\n",
      "Task : 0 \t Episode: 160 \t Time_step: 199 \t Avg_reward: -9.02936969613 \t Cur_reward: -7.88783701727\n",
      "Task : 0 \t Episode: 161 \t Time_step: 199 \t Avg_reward: -8.97905640461 \t Cur_reward: -9.86350037303\n",
      "Task : 0 \t Episode: 162 \t Time_step: 199 \t Avg_reward: -8.87196064427 \t Cur_reward: -6.85674617958\n",
      "Task : 0 \t Episode: 163 \t Time_step: 199 \t Avg_reward: -8.82170974698 \t Cur_reward: -7.69721070927\n",
      "Task : 0 \t Episode: 164 \t Time_step: 199 \t Avg_reward: -8.78441252897 \t Cur_reward: -6.62262216034\n",
      "Task : 0 \t Episode: 165 \t Time_step: 199 \t Avg_reward: -8.72439317145 \t Cur_reward: -5.26758574396\n",
      "Task : 0 \t Episode: 166 \t Time_step: 199 \t Avg_reward: -8.63122508128 \t Cur_reward: -5.38265176051\n",
      "Task : 0 \t Episode: 167 \t Time_step: 199 \t Avg_reward: -8.59221032644 \t Cur_reward: -7.70926887012\n",
      "Task : 0 \t Episode: 168 \t Time_step: 199 \t Avg_reward: -8.53057071737 \t Cur_reward: -8.2346000265\n",
      "Task : 0 \t Episode: 169 \t Time_step: 199 \t Avg_reward: -8.46343799647 \t Cur_reward: -6.78169908735\n",
      "Task : 0 \t Episode: 170 \t Time_step: 199 \t Avg_reward: -8.40377421653 \t Cur_reward: -4.2469544995\n",
      "Task : 0 \t Episode: 171 \t Time_step: 199 \t Avg_reward: -8.35811691144 \t Cur_reward: -5.47884922601\n",
      "Task : 0 \t Episode: 172 \t Time_step: 199 \t Avg_reward: -8.24166434867 \t Cur_reward: -6.30650556997\n",
      "Task : 0 \t Episode: 173 \t Time_step: 199 \t Avg_reward: -8.16172440365 \t Cur_reward: -6.993120061\n",
      "Task : 0 \t Episode: 174 \t Time_step: 199 \t Avg_reward: -8.14069564722 \t Cur_reward: -5.77081125207\n",
      "Task : 0 \t Episode: 175 \t Time_step: 199 \t Avg_reward: -8.07407426498 \t Cur_reward: -5.93835203213\n",
      "Task : 0 \t Episode: 176 \t Time_step: 199 \t Avg_reward: -8.06871158273 \t Cur_reward: -7.86876780884\n",
      "Task : 0 \t Episode: 177 \t Time_step: 199 \t Avg_reward: -8.07110427336 \t Cur_reward: -6.98778105606\n",
      "Task : 0 \t Episode: 178 \t Time_step: 199 \t Avg_reward: -8.0364007239 \t Cur_reward: -5.09295179723\n",
      "Task : 0 \t Episode: 179 \t Time_step: 199 \t Avg_reward: -8.01076191996 \t Cur_reward: -5.89279636017\n",
      "Task : 0 \t Episode: 180 \t Time_step: 199 \t Avg_reward: -7.92369193182 \t Cur_reward: -4.92375163801\n",
      "Task : 0 \t Episode: 181 \t Time_step: 199 \t Avg_reward: -7.89403773831 \t Cur_reward: -5.49690331502\n",
      "Task : 0 \t Episode: 182 \t Time_step: 199 \t Avg_reward: -7.8439396901 \t Cur_reward: -4.63420480084\n",
      "Task : 0 \t Episode: 183 \t Time_step: 199 \t Avg_reward: -7.76077904257 \t Cur_reward: -5.41020910463\n",
      "Task : 0 \t Episode: 184 \t Time_step: 199 \t Avg_reward: -7.74971325438 \t Cur_reward: -6.37672113651\n",
      "Task : 0 \t Episode: 185 \t Time_step: 199 \t Avg_reward: -7.67452178242 \t Cur_reward: -5.14690082839\n",
      "Task : 0 \t Episode: 186 \t Time_step: 199 \t Avg_reward: -7.61183935359 \t Cur_reward: -5.69997106217\n",
      "Task : 0 \t Episode: 187 \t Time_step: 199 \t Avg_reward: -7.57820256384 \t Cur_reward: -5.66794088549\n",
      "Task : 0 \t Episode: 188 \t Time_step: 199 \t Avg_reward: -7.56217486135 \t Cur_reward: -6.56753333476\n",
      "Task : 0 \t Episode: 189 \t Time_step: 199 \t Avg_reward: -7.48593914293 \t Cur_reward: -6.70962554681\n",
      "Task : 0 \t Episode: 190 \t Time_step: 199 \t Avg_reward: -7.46640191386 \t Cur_reward: -6.75886178548\n",
      "Task : 0 \t Episode: 191 \t Time_step: 199 \t Avg_reward: -7.44460255623 \t Cur_reward: -5.82283379034\n",
      "Task : 0 \t Episode: 192 \t Time_step: 199 \t Avg_reward: -7.4084946342 \t Cur_reward: -3.34603084382\n",
      "Task : 0 \t Episode: 193 \t Time_step: 199 \t Avg_reward: -7.42014263955 \t Cur_reward: -6.14973059524\n",
      "Task : 0 \t Episode: 194 \t Time_step: 199 \t Avg_reward: -7.41645834838 \t Cur_reward: -7.04780662687\n",
      "Task : 0 \t Episode: 195 \t Time_step: 199 \t Avg_reward: -7.39961867138 \t Cur_reward: -6.17970443666\n",
      "Task : 0 \t Episode: 196 \t Time_step: 199 \t Avg_reward: -7.36185613226 \t Cur_reward: -5.22177771206\n",
      "Task : 0 \t Episode: 197 \t Time_step: 199 \t Avg_reward: -7.34173771465 \t Cur_reward: -6.72024595358\n",
      "Task : 0 \t Episode: 198 \t Time_step: 199 \t Avg_reward: -7.35082049388 \t Cur_reward: -7.21071561083\n",
      "Task : 0 \t Episode: 199 \t Time_step: 199 \t Avg_reward: -7.31736962267 \t Cur_reward: -5.44490669359\n",
      "Task : 0 \t Episode: 200 \t Time_step: 199 \t Avg_reward: -7.31076457695 \t Cur_reward: -5.21520876673\n",
      "Task : 0 \t Episode: 201 \t Time_step: 199 \t Avg_reward: -7.31817539138 \t Cur_reward: -6.2572351079\n",
      "Task : 0 \t Episode: 202 \t Time_step: 199 \t Avg_reward: -7.34597185621 \t Cur_reward: -8.78346741529\n",
      "Task : 0 \t Episode: 203 \t Time_step: 199 \t Avg_reward: -7.3421355764 \t Cur_reward: -5.5462202366\n",
      "Task : 0 \t Episode: 204 \t Time_step: 199 \t Avg_reward: -7.33273000098 \t Cur_reward: -4.70844980043\n",
      "Task : 0 \t Episode: 205 \t Time_step: 199 \t Avg_reward: -7.31874580081 \t Cur_reward: -4.90898735799\n",
      "Task : 0 \t Episode: 206 \t Time_step: 199 \t Avg_reward: -7.32797814378 \t Cur_reward: -7.77848132354\n",
      "Task : 0 \t Episode: 207 \t Time_step: 199 \t Avg_reward: -7.32835950073 \t Cur_reward: -6.02056206369\n",
      "Task : 0 \t Episode: 208 \t Time_step: 199 \t Avg_reward: -7.32436649422 \t Cur_reward: -4.20948480019\n",
      "Task : 0 \t Episode: 209 \t Time_step: 199 \t Avg_reward: -7.39396710852 \t Cur_reward: -11.5061737435\n",
      "Task : 0 \t Episode: 210 \t Time_step: 199 \t Avg_reward: -7.37356652896 \t Cur_reward: -4.88936079101\n",
      "Task : 0 \t Episode: 211 \t Time_step: 199 \t Avg_reward: -7.36768160744 \t Cur_reward: -4.58930549227\n",
      "Task : 0 \t Episode: 212 \t Time_step: 199 \t Avg_reward: -7.33179288256 \t Cur_reward: -4.46775806594\n",
      "Task : 0 \t Episode: 213 \t Time_step: 199 \t Avg_reward: -7.33112139155 \t Cur_reward: -5.95212359672\n",
      "Task : 0 \t Episode: 214 \t Time_step: 199 \t Avg_reward: -7.33563898632 \t Cur_reward: -6.32241755415\n",
      "Task : 0 \t Episode: 215 \t Time_step: 199 \t Avg_reward: -7.33339214105 \t Cur_reward: -4.88664087718\n",
      "Task : 0 \t Episode: 216 \t Time_step: 199 \t Avg_reward: -7.34950488254 \t Cur_reward: -6.12263434292\n",
      "Task : 0 \t Episode: 217 \t Time_step: 199 \t Avg_reward: -7.36432849777 \t Cur_reward: -5.91857486581\n",
      "Task : 0 \t Episode: 218 \t Time_step: 199 \t Avg_reward: -7.33464102122 \t Cur_reward: -4.86704734208\n",
      "Task : 0 \t Episode: 219 \t Time_step: 199 \t Avg_reward: -7.32619517944 \t Cur_reward: -4.68550489678\n",
      "Task : 0 \t Episode: 220 \t Time_step: 199 \t Avg_reward: -7.31019162192 \t Cur_reward: -5.09102691967\n",
      "Task : 0 \t Episode: 221 \t Time_step: 199 \t Avg_reward: -7.29355878902 \t Cur_reward: -5.30215331377\n",
      "Task : 0 \t Episode: 222 \t Time_step: 199 \t Avg_reward: -7.27707758969 \t Cur_reward: -4.77466656317\n",
      "Task : 0 \t Episode: 223 \t Time_step: 199 \t Avg_reward: -7.25899192939 \t Cur_reward: -5.35034409025\n",
      "Task : 0 \t Episode: 224 \t Time_step: 199 \t Avg_reward: -7.19895062337 \t Cur_reward: -4.20782904278\n",
      "Task : 0 \t Episode: 225 \t Time_step: 199 \t Avg_reward: -7.11826449558 \t Cur_reward: -4.6931393549\n",
      "Task : 0 \t Episode: 226 \t Time_step: 199 \t Avg_reward: -7.0867536225 \t Cur_reward: -3.26696070786\n",
      "Task : 0 \t Episode: 227 \t Time_step: 199 \t Avg_reward: -7.04569031192 \t Cur_reward: -4.35335449909\n",
      "Task : 0 \t Episode: 228 \t Time_step: 199 \t Avg_reward: -6.99147942656 \t Cur_reward: -2.75664079807\n",
      "Task : 0 \t Episode: 229 \t Time_step: 199 \t Avg_reward: -6.94511371212 \t Cur_reward: -5.11544012837\n",
      "Task : 0 \t Episode: 230 \t Time_step: 199 \t Avg_reward: -6.8930435505 \t Cur_reward: -3.33669738939\n",
      "Task : 0 \t Episode: 231 \t Time_step: 199 \t Avg_reward: -6.83442059684 \t Cur_reward: -3.35890547685\n",
      "Task : 0 \t Episode: 232 \t Time_step: 199 \t Avg_reward: -6.77374980175 \t Cur_reward: -4.49580580966\n",
      "Task : 0 \t Episode: 233 \t Time_step: 199 \t Avg_reward: -6.71306145745 \t Cur_reward: -3.004699604\n",
      "Task : 0 \t Episode: 234 \t Time_step: 162 \t Avg_reward: -6.63277212653 \t Cur_reward: -3.02614754174\n",
      "Task : 0 \t Episode: 235 \t Time_step: 199 \t Avg_reward: -6.55434245007 \t Cur_reward: -3.20731657746\n",
      "Task : 0 \t Episode: 236 \t Time_step: 199 \t Avg_reward: -6.45501504358 \t Cur_reward: -3.19944395926\n",
      "Task : 0 \t Episode: 237 \t Time_step: 199 \t Avg_reward: -6.33554161321 \t Cur_reward: -3.25760309254\n",
      "Task : 0 \t Episode: 238 \t Time_step: 199 \t Avg_reward: -6.26320218976 \t Cur_reward: -3.01067060818\n",
      "Task : 0 \t Episode: 239 \t Time_step: 199 \t Avg_reward: -6.19439071866 \t Cur_reward: -3.23211792948\n",
      "Task : 0 \t Episode: 240 \t Time_step: 199 \t Avg_reward: -6.11374170573 \t Cur_reward: -3.93888751494\n",
      "Task : 0 \t Episode: 241 \t Time_step: 199 \t Avg_reward: -6.03070176299 \t Cur_reward: -3.38166867062\n",
      "Task : 0 \t Episode: 242 \t Time_step: 199 \t Avg_reward: -5.92088020978 \t Cur_reward: -2.53470361364\n",
      "Task : 0 \t Episode: 243 \t Time_step: 199 \t Avg_reward: -5.86914873508 \t Cur_reward: -4.1287775965\n",
      "Task : 0 \t Episode: 244 \t Time_step: 199 \t Avg_reward: -5.83683970361 \t Cur_reward: -3.45557181428\n",
      "Task : 0 \t Episode: 245 \t Time_step: 169 \t Avg_reward: -5.79671783024 \t Cur_reward: -3.76209268136\n",
      "Task : 0 \t Episode: 246 \t Time_step: 199 \t Avg_reward: -5.72512120588 \t Cur_reward: -3.09469637081\n",
      "Task : 0 \t Episode: 247 \t Time_step: 199 \t Avg_reward: -5.67512668535 \t Cur_reward: -3.91062823411\n",
      "Task : 0 \t Episode: 248 \t Time_step: 199 \t Avg_reward: -5.60953962566 \t Cur_reward: -2.74040524642\n",
      "Task : 0 \t Episode: 249 \t Time_step: 195 \t Avg_reward: -5.53758682549 \t Cur_reward: -3.11608272048\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (250) into shape (1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b8fba9c7e84e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Task : %s \\t Episode: %s \\t Time_step: %s \\t Avg_reward: %s \\t Cur_reward: %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_avg_reward_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sum_reward_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0msummary_sum_reward_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sum_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0msummary_avg_reward_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_avg_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mrecord_paras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'actor_target'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (250) into shape (1000)"
     ]
    }
   ],
   "source": [
    "for j in range(len(goal_pos)-48):\n",
    "    env.set_goal(np.append(goal_pos[j], [0,0]))\n",
    "    all_sum_reward_list = []\n",
    "    all_avg_reward_list = []\n",
    "    all_reward_list = []\n",
    "    all_loss_list = []\n",
    "    all_t_list = []\n",
    "    \n",
    "    for i in range(MAX_EPISODE):\n",
    "\n",
    "        state = env.reset()\n",
    "        noise.reset()\n",
    "        reward_list = []\n",
    "        loss_list = []\n",
    "\n",
    "        for t in range(MAX_TIME):\n",
    "            action = actor.predict(np.reshape(state, (-1, state_dim))) \n",
    "            action += noise.noise()\n",
    "            action = np.clip(action, -action_bound, action_bound)\n",
    "            action = np.reshape(action, action_dim)\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            replay_buffer.add_sample(np.reshape(state, state_dim), \\\n",
    "                                     np.reshape(action,action_dim),\\\n",
    "                                     reward,\\\n",
    "                                     np.reshape(next_state,state_dim),\\\n",
    "                                     done)\n",
    "\n",
    "            mini_batch = replay_buffer.rand_sample(batch_size = BATCH_SIZE, seed = RANDOM_SEED + t + i*MAX_TIME)\n",
    "            s_batch, a_batch, r_batch, s2_batch, t_batch = mini_batch\n",
    "\n",
    "            a2_batch = actor.predict(s2_batch, if_target = True)\n",
    "            training_q = r_batch + GAMMA * critic.predict(s2_batch, a2_batch, if_target = True) #* ~t_batch\n",
    "\n",
    "            _, loss = critic.train(s_batch, a_batch, training_q)\n",
    "\n",
    "            train_action_batch = actor.predict(s_batch)\n",
    "            critic_grad = critic.compute_critic_gradient(s_batch, train_action_batch)\n",
    "            actor.train(s_batch, critic_grad[0])\n",
    "\n",
    "            actor.update_target_network()\n",
    "            critic.update_target_network()\n",
    "\n",
    "            reward_list.append(reward)\n",
    "            loss_list.append(loss)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    #         print('Episode: %s \\t Action: %s, %s \\t State: %s,%s,%s,%s' %(i, action[0], action[1], state[0],state[1],state[2],state[3]))\n",
    "\n",
    "        all_sum_reward_list.append(np.sum(reward_list))\n",
    "        all_avg_reward_list.append(np.mean(all_sum_reward_list[-100:]))\n",
    "        all_reward_list.append(reward_list)\n",
    "        all_loss_list.append(loss_list)\n",
    "        all_t_list.append(t)\n",
    "\n",
    "        print('Task : %s \\t Episode: %s \\t Time_step: %s \\t Avg_reward: %s \\t Cur_reward: %s'%(j, i, t, all_avg_reward_list[-1], all_sum_reward_list[-1]))\n",
    "    \n",
    "    summary_sum_reward_list[j] = np.array(all_sum_reward_list)\n",
    "    summary_avg_reward_list[j] = np.array(all_avg_reward_list)\n",
    "    record_paras = [v for v in tf.trainable_variables() if 'actor_target' in v.name]\n",
    "    all_recorded_paras.append(record_paras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(all_avg_reward_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testing = [v for v in tf.trainable_variables() if 'actor_target' in v.name]\n",
    "# test,test2 = sess.run([tf.trainable_variables(), testing])\n",
    "test = sess.run(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 40)\n",
      "(40,)\n",
      "(40, 30)\n",
      "(30,)\n",
      "(30, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(np.shape(v)) for v in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 40)\n",
      "(40,)\n",
      "(40, 30)\n",
      "(30,)\n",
      "(30, 2)\n",
      "(5, 40)\n",
      "(40,)\n",
      "(40, 30)\n",
      "(30,)\n",
      "(30, 2)\n",
      "(5, 30)\n",
      "(2, 30)\n",
      "(30, 1)\n",
      "(30,)\n",
      "(5, 30)\n",
      "(2, 30)\n",
      "(30, 1)\n",
      "(30,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(np.shape(v)) for v in test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
