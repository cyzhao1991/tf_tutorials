{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from actor import ActorNetwork\n",
    "from rbfActor import RbfActorNetwork\n",
    "from critic import CriticNetwork\n",
    "from replay_buffer import ReplayBuffer\n",
    "from ounoise import OUNoise\n",
    "import gym, time\n",
    "from Envs.reaching import ReachingEnv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "MAX_EPISODE = 1000\n",
    "MAX_TIME = 200\n",
    "\n",
    "ACTOR_LEARNING_RATE = 0.0001\n",
    "CRITIC_LEARNING_RATE = 0.001\n",
    "L2_DECAY = 0.01\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "\n",
    "BUFFER_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "OU_MU = 0.0\n",
    "OU_THETA = 0.15  \n",
    "OU_SIGMA = 0.20\n",
    "\n",
    "RANDOM_SEED = 1926\n",
    "\n",
    "goal_pos = np.array([[-1.7691047 , -1.76426373],\n",
    "       [-1.81476041,  1.25572033],\n",
    "       [ 1.97538345,  1.7239961 ],\n",
    "       [ 0.49885795,  1.82511657],\n",
    "       [-1.45703216,  1.39941234],\n",
    "       [ 0.49100693,  0.12822174],\n",
    "       [ 0.14809867, -1.31716354],\n",
    "       [-1.30413931,  1.79390377],\n",
    "       [-0.60087002,  1.90940639],\n",
    "       [ 1.29273111,  1.85736147]])\n",
    "\n",
    "# env = gym.make('Pendulum-v0')\n",
    "env = ReachingEnv(include_t = True)\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = env.action_space.high\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# actor = ActorNetwork(sess, state_dim, action_dim, action_bound, hidden_layer_dim = [40,30], \\\n",
    "#                      seed = RANDOM_SEED, tau = TAU, learning_rate = ACTOR_LEARNING_RATE)\n",
    "actor = RbfActorNetwork(sess, state_dim, action_dim, action_bound, hidden_layer_dim = [40,30], \\\n",
    "                     seed = RANDOM_SEED, tau = TAU, learning_rate = ACTOR_LEARNING_RATE)\n",
    "\n",
    "critic = CriticNetwork(sess, state_dim, action_dim, hidden_layer_dim = [30],\\\n",
    "                       l2_alpha = L2_DECAY, seed = RANDOM_SEED, tau =TAU, learning_rate = CRITIC_LEARNING_RATE)\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "noise = OUNoise(action_dim, mu = OU_MU, theta = OU_THETA, sigma = OU_SIGMA, seed = RANDOM_SEED)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "summary_sum_reward_list = np.zeros([10,1000])\n",
    "summary_avg_reward_list = np.zeros([10,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-21911e35df67>, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-21911e35df67>\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    all_reward_list.append(reward_list)\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(goal_pos)):\n",
    "    env.set_goal(np.append(goal_pos[j], [0,0]))\n",
    "    all_sum_reward_list = []\n",
    "    all_avg_reward_list = []\n",
    "    all_reward_list = []\n",
    "    all_loss_list = []\n",
    "    all_t_list = []\n",
    "    \n",
    "    for i in range(MAX_EPISODE):\n",
    "\n",
    "        state = env.reset()\n",
    "        noise.reset()\n",
    "        reward_list = []\n",
    "        loss_list = []\n",
    "\n",
    "        for t in range(MAX_TIME):\n",
    "            action = actor.predict(np.reshape(state, (-1, state_dim))) \n",
    "            action += noise.noise()\n",
    "            action = np.clip(action, -action_bound, action_bound)\n",
    "            action = np.reshape(action, action_dim)\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            replay_buffer.add_sample(np.reshape(state, state_dim), \\\n",
    "                                     np.reshape(action,action_dim),\\\n",
    "                                     reward,\\\n",
    "                                     np.reshape(next_state,state_dim),\\\n",
    "                                     t)\n",
    "\n",
    "            mini_batch = replay_buffer.rand_sample(batch_size = BATCH_SIZE, seed = RANDOM_SEED + t + i*MAX_TIME)\n",
    "            s_batch, a_batch, r_batch, s2_batch, t_batch = mini_batch\n",
    "\n",
    "            a2_batch = actor.predict(s2_batch, if_target = True)\n",
    "            training_q = r_batch + GAMMA * critic.predict(s2_batch, a2_batch, if_target = True)\n",
    "\n",
    "            _, loss = critic.train(s_batch, a_batch, training_q)\n",
    "\n",
    "            train_action_batch = actor.predict(s_batch)\n",
    "            critic_grad = critic.compute_critic_gradient(s_batch, train_action_batch)\n",
    "            actor.train(s_batch, critic_grad[0])\n",
    "\n",
    "            actor.update_target_network()\n",
    "            critic.update_target_network()\n",
    "\n",
    "            reward_list.append(reward)\n",
    "            loss_list.append(loss)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    #         print('Episode: %s \\t Action: %s, %s \\t State: %s,%s,%s,%s' %(i, action[0], action[1], state[0],state[1],state[2],state[3]))\n",
    "\n",
    "        all_sum_reward_list.append(np.sum(reward_list))\n",
    "        all_avg_reward_list.append(np.mean(all_sum_reward_list[-100:]))\n",
    "        all_reward_list.append(reward_list)\n",
    "        all_loss_list.append(loss_list)\n",
    "        all_t_list.append(t)\n",
    "\n",
    "        print('Task : %s \\t Episode: %s \\t Time_step: %s \\t Avg_reward: %s \\t Cur_reward: %s'%(j, i, t, all_avg_reward_list[-1], all_sum_reward_list[-1]))\n",
    "    \n",
    "    summary_sum_reward_list[j] = np.array(all_sum_reward_list)\n",
    "    summary_avg_reward_list[j] = np.array(all_avg_reward_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot(all_avg_reward_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
